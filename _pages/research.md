---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

{% if site.talkmap_link == true %}

<p style="text-decoration:underline;"><a href="/research.md">See a map of all the places I've given a talk!</a></p>

{% endif %}

### Assessment of Large Language Models’ Ability to Generate Relevant and High-Quality Questions at Different Bloom’s Skill Levels

Creating pedagogically effective questions that promote learning is a difficult task for teachers, and requires a lot of time and careful planning. Modern large language models (LLMs) can generate questions to help teachers and enable students to self-assess. However, it is important to evaluate the quality and relevance of these questions. In this study, we examined the ability of five state-of-the-art LLMs to generate relevant and high-quality questions of different cognitive levels, as defined by Bloom's taxonomy. The five LLMs used were instruction-tuned versions of Bard, Falcon-40B, GPT 3.5, GPT 4, and Llama 2 70B. We prompted each model with the same instructions and different contexts to generate 510 questions. Two human experts used a ten-item rubric to assess the linguistic and pedagogical relevance and quality of the questions. Our results showed that 98.03% of the LLM-generated questions were relevant and 91.5% were of high quality. We also investigated the performance of the five LLMs in generating questions of different cognitive levels. Our findings suggest that LLMs can generate relevant and high-quality educational questions of different cognitive levels, making them useful for creating assessments.